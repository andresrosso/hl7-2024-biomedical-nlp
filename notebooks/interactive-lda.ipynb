{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"Establish a connection to the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def load_table_to_dataframe(conn, table_name):\n",
    "    \"\"\"Load a table from the SQLite database into a Pandas DataFrame.\"\"\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "\n",
    "def create_text_column(df):\n",
    "    \"\"\"Create a new column 'TEXT' by concatenating specific columns.\"\"\"\n",
    "    df['TEXT'] = df['title'] + ' ' + df['meta_description'] + ' ' + df['description'] + ' ' + df['body']\n",
    "    df['TEXT'] = df['TEXT'].fillna('')\n",
    "    return df\n",
    "\n",
    "# DB path\n",
    "db_path = r\"../data/articles.sqlite\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "table_name = \"article\"\n",
    "df = load_table_to_dataframe(conn, table_name)\n",
    "\n",
    "# Create the 'TEXT' column\n",
    "df = create_text_column(df)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from dateutil import parser\n",
    "import os\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Custom function to parse the Spanish date strings\n",
    "def parse_spanish_date(date_str):\n",
    "    # Handle None or NaN values\n",
    "    if pd.isnull(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Replace month names in Spanish with numbers\n",
    "    months = {\n",
    "        \"enero\": \"01\",\n",
    "        \"febrero\": \"02\",\n",
    "        \"marzo\": \"03\",\n",
    "        \"abril\": \"04\",\n",
    "        \"mayo\": \"05\",\n",
    "        \"junio\": \"06\",\n",
    "        \"julio\": \"07\",\n",
    "        \"agosto\": \"08\",\n",
    "        \"septiembre\": \"09\",\n",
    "        \"octubre\": \"10\",\n",
    "        \"noviembre\": \"11\",\n",
    "        \"diciembre\": \"12\"\n",
    "    }\n",
    "    \n",
    "    date_str = date_str.lower()\n",
    "    \n",
    "    for month, num in months.items():\n",
    "        date_str = date_str.replace(month, num)\n",
    "    \n",
    "    # Now try to parse the date using dateutil.parser\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_str, dayfirst=True)\n",
    "        # Remove timezone info if present\n",
    "        if parsed_date.tzinfo is not None:\n",
    "            parsed_date = parsed_date.replace(tzinfo=None)\n",
    "    except ValueError:\n",
    "        parsed_date = pd.NaT  # Not a Time if parsing fails\n",
    "    \n",
    "    return parsed_date\n",
    "\n",
    "# Apply the custom parsing function to the 'date' column\n",
    "df['DATE'] = df['date'].apply(parse_spanish_date)\n",
    "\n",
    "preprocessed_texts = df['TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Define the path to save the LDA model\n",
    "lda_model_path = \"lda_model_interactive.gensim\"\n",
    "\n",
    "# Check if the model already exists, if so, load it; otherwise, train and save it\n",
    "if os.path.exists(lda_model_path):\n",
    "    lda_model = models.LdaModel.load(lda_model_path)\n",
    "    print(\"LDA model loaded from disk.\")\n",
    "else:\n",
    "    lda_model = models.LdaModel(corpus, num_topics=7, id2word=dictionary, passes=100)\n",
    "    lda_model.save(lda_model_path)\n",
    "    print(\"LDA model trained and saved to disk.\")\n",
    "\n",
    "# Label topics with the 2 most frequent bigrams\n",
    "def identify_topics_by_frequent_bigrams(lda_model, corpus, preprocessed_texts, num_bigrams=2):\n",
    "    \"\"\"Identify each topic by the most frequent bigrams within the documents associated with that topic.\"\"\"\n",
    "    document_topics = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "    grouped_documents = {topic: [] for topic in range(lda_model.num_topics)}\n",
    "    for doc_id, topic in enumerate(document_topics):\n",
    "        grouped_documents[topic].append(doc_id)\n",
    "    \n",
    "    # Extract the most frequent bigrams for each topic\n",
    "    topic_labels = {}\n",
    "    for topic, documents in grouped_documents.items():\n",
    "        group_texts = [preprocessed_texts[doc_id] for doc_id in documents]\n",
    "        all_bigrams = [bigram for text in group_texts for bigram in ngrams(text, 2)]\n",
    "        bigram_freq = Counter(all_bigrams)\n",
    "        most_common_bigrams = [' '.join(bigram) for bigram, _ in bigram_freq.most_common(num_bigrams)]\n",
    "        topic_labels[topic] = ' / '.join(most_common_bigrams)\n",
    "    \n",
    "    return topic_labels, grouped_documents\n",
    "\n",
    "topic_labels, grouped_documents = identify_topics_by_frequent_bigrams(lda_model, corpus, preprocessed_texts)\n",
    "\n",
    "# Prepare data for plotting\n",
    "topic_data = []\n",
    "for doc_id, row in df.iterrows():\n",
    "    topics = lda_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_id, prob in topics:\n",
    "        topic_data.append({\n",
    "            \"Document\": doc_id,\n",
    "            \"Topic\": topic_labels[topic_id],\n",
    "            \"Probability\": prob,\n",
    "            \"Date\": row['DATE']\n",
    "        })\n",
    "\n",
    "topic_df = pd.DataFrame(topic_data)\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "topic_df['Date'] = pd.to_datetime(topic_df['Date'])\n",
    "\n",
    "# Group by month and calculate the average topic distribution\n",
    "topic_df['Month'] = topic_df['Date'].dt.to_period('M').astype(str)\n",
    "monthly_avg_topic_df = topic_df.groupby(['Month', 'Topic']).agg({'Probability': 'mean'}).reset_index()\n",
    "\n",
    "# Dash app setup\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Topic Exploration\"),\n",
    "    \n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[{'label': topic_labels[i], 'value': topic_labels[i]} for i in range(lda_model.num_topics)],\n",
    "        value=topic_labels[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    \n",
    "    dcc.Graph(id='topic-graph'),\n",
    "    \n",
    "    html.Div(id='topic-words')\n",
    "])\n",
    "\n",
    "# Callback to update the graph and the list of words for a selected topic\n",
    "@app.callback(\n",
    "    [Output('topic-graph', 'figure'),\n",
    "     Output('topic-words', 'children')],\n",
    "    [Input('topic-dropdown', 'value')]\n",
    ")\n",
    "def update_graph(selected_topic):\n",
    "    filtered_df = monthly_avg_topic_df[monthly_avg_topic_df['Topic'] == selected_topic]\n",
    "    \n",
    "    fig = px.bar(filtered_df, x='Month', y='Probability',\n",
    "                 title=f'Average Monthly Distribution for Topic {selected_topic}',\n",
    "                 labels={\"Probability\": \"Average Probability\"})\n",
    "    \n",
    "    # Get the top words for the selected topic\n",
    "    topic_id = list(topic_labels.keys())[list(topic_labels.values()).index(selected_topic)]\n",
    "    top_words = lda_model.show_topic(topic_id, topn=10)\n",
    "    word_list = [f\"{word}: {round(prob, 3)}\" for word, prob in top_words]\n",
    "    word_div = html.Div([\n",
    "        html.H3(\"Most Frequent Words\"),\n",
    "        html.Ul([html.Li(word) for word in word_list])\n",
    "    ])\n",
    "    \n",
    "    return fig, word_div\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hl7-speak2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
