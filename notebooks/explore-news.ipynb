{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'url', 'title', 'meta_description', 'description', 'date', 'tags',\n",
      "       'author', 'body', 'status', 'TEXT'],\n",
      "      dtype='object')\n",
      "   id                                                url  \\\n",
      "0   1  https://www.elespectador.com/salud/no-es-una-s...   \n",
      "1   2  https://www.elespectador.com/salud/corte-orden...   \n",
      "2   3  https://www.elespectador.com/politica/la-adver...   \n",
      "3   4  https://www.elespectador.com/salud/supersalud-...   \n",
      "4   5  https://www.elespectador.com/salud/reforma-a-l...   \n",
      "\n",
      "                                               title  \\\n",
      "0  No es una, son tres las reformas a la salud qu...   \n",
      "1  Corte ordena al Minsalud pagar saldos pendient...   \n",
      "2  “El que no haga caso, se va”: la advertencia d...   \n",
      "3  Supersalud interviene a la EPS Sanitas, con má...   \n",
      "4  Reforma a la salud: aprobaron el 49%, pero fal...   \n",
      "\n",
      "                                    meta_description  \\\n",
      "0  A la propuesta del Gobierno se suma una de la ...   \n",
      "1  El Ministerio de Salud tendrá un plazo de dos ...   \n",
      "2  El mandatario aseguró que sus ministros deberá...   \n",
      "3  Sanitas es una EPS cuyo propietario es Keralty...   \n",
      "4  Se aprobó casi la mitad del proyecto, pero agu...   \n",
      "\n",
      "                                         description  \\\n",
      "0  El debate en las comisiones séptimas del legis...   \n",
      "1  El Ministerio de Salud tendrá dos meses para p...   \n",
      "2  El mandatario aseguró que sus ministros deberá...   \n",
      "3  La EPS ha estado en el centro del debate duran...   \n",
      "4  Tras jornadas de más de 7 horas, la plenaria d...   \n",
      "\n",
      "                                  date  \\\n",
      "0  16 de febrero de 2023 - 12:00 p. m.   \n",
      "1  26 de febrero de 2024 - 01:01 p. m.   \n",
      "2    07 de junio de 2023 - 05:05 p. m.   \n",
      "3    02 de abril de 2024 - 06:04 p. m.   \n",
      "4  11 de octubre de 2023 - 01:25 p. m.   \n",
      "\n",
      "                                                tags              author  \\\n",
      "0                        Reforma a la salud, Premium     Redacción Salud   \n",
      "1                Noticias hoy, Noticias hoy Colombia     Redacción Salud   \n",
      "2  política, noticias, Noticias de Política, Colo...  Redacción Política   \n",
      "3  Noticias hoy, Noticias hoy Colombia, EPS inter...     Redacción Salud   \n",
      "4  Reforma a la salud, sistema de salud, aprueban...     Redacción Salud   \n",
      "\n",
      "                                                body     status  \\\n",
      "0  Los tres proyectos comenzarán discusiones desd...  completed   \n",
      "1  Guillermo Jaramillo, ministro de Salud.Foto: Ó...  completed   \n",
      "2  El presidente de Colombia Petro, acompañado po...  completed   \n",
      "3  La Superintendencia Nacional de Salud tomó la ...  completed   \n",
      "4  Presentación y radicación de la reforma a la s...  completed   \n",
      "\n",
      "                                                TEXT  \n",
      "0  No es una, son tres las reformas a la salud qu...  \n",
      "1  Corte ordena al Minsalud pagar saldos pendient...  \n",
      "2  “El que no haga caso, se va”: la advertencia d...  \n",
      "3  Supersalud interviene a la EPS Sanitas, con má...  \n",
      "4  Reforma a la salud: aprobaron el 49%, pero fal...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "def connect_to_db(db_path):\n",
    "    \"\"\"Establish a connection to the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    return conn\n",
    "\n",
    "def load_table_to_dataframe(conn, table_name):\n",
    "    \"\"\"Load a table from the SQLite database into a Pandas DataFrame.\"\"\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    df = pd.read_sql(query, conn)\n",
    "    return df\n",
    "\n",
    "def create_text_column(df):\n",
    "    \"\"\"Create a new column 'TEXT' by concatenating specific columns.\"\"\"\n",
    "    df['TEXT'] = df['title'] + ' ' + df['meta_description'] + ' ' + df['description'] + ' ' + df['body']\n",
    "    df['TEXT'] = df['TEXT'].fillna('')\n",
    "    return df\n",
    "\n",
    "# DB path\n",
    "db_path = r\"../data/articles.sqlite\"\n",
    "\n",
    "# Connect to the database\n",
    "conn = connect_to_db(db_path)\n",
    "\n",
    "# Load the table into a DataFrame\n",
    "table_name = \"article\"\n",
    "df = load_table_to_dataframe(conn, table_name)\n",
    "\n",
    "# Create the 'TEXT' column\n",
    "df = create_text_column(df)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df.columns)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentación del Código\n",
    "\n",
    "Este script implementa una aplicación web interactiva utilizando Dash para explorar artículos de noticias agrupados por temas utilizando un modelo LDA (Latent Dirichlet Allocation). La aplicación permite a los usuarios seleccionar un tema y visualizar los artículos relacionados con dicho tema, ordenados por fecha.\n",
    "\n",
    "## Librerías Utilizadas\n",
    "\n",
    "- `dash`: Framework para crear aplicaciones web interactivas en Python.\n",
    "- `plotly.express`: Utilizado para crear gráficos interactivos.\n",
    "- `pandas`: Utilizado para la manipulación de datos en DataFrames.\n",
    "- `gensim`: Utilizado para el procesamiento de texto y modelado de tópicos LDA.\n",
    "- `nltk`: Utilizado para la tokenización, eliminación de stopwords y lematización.\n",
    "- `dateutil`: Utilizado para parsear fechas en diferentes formatos.\n",
    "- `os`: Utilizado para interactuar con el sistema de archivos.\n",
    "\n",
    "## Preprocesamiento del Texto\n",
    "\n",
    "### `preprocess_text(text)`\n",
    "Esta función realiza el preprocesamiento del texto, que incluye:\n",
    "\n",
    "- **Tokenización:** Divide el texto en palabras (tokens).\n",
    "- **Filtrado:** Elimina tokens que no sean alfabéticos y stopwords en español.\n",
    "- **Lematización:** Reduce cada palabra a su forma base.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `text` (str): Texto a preprocesar.\n",
    "  \n",
    "- **Retorna:**\n",
    "  - `tokens` (list): Lista de tokens preprocesados.\n",
    "\n",
    "## Parseo de Fechas en Español\n",
    "\n",
    "### `parse_spanish_date(date_str)`\n",
    "Esta función convierte una cadena de texto con fecha en español a un objeto de fecha en formato `datetime`.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `date_str` (str): Cadena de texto que contiene la fecha en español.\n",
    "  \n",
    "- **Retorna:**\n",
    "  - `parsed_date` (datetime): Objeto `datetime` que representa la fecha parseada.\n",
    "\n",
    "## Creación del Modelo LDA\n",
    "\n",
    "### `identify_topics_by_frequent_bigrams(lda_model, corpus, preprocessed_texts, num_bigrams=2)`\n",
    "Identifica los temas generados por el modelo LDA mediante los bigramas más frecuentes en los documentos asociados a cada tema.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `lda_model` (gensim.models.LdaModel): Modelo LDA entrenado.\n",
    "  - `corpus` (list): Corpus en formato BoW (Bag of Words) de los documentos.\n",
    "  - `preprocessed_texts` (pandas.Series): Serie de textos preprocesados.\n",
    "  - `num_bigrams` (int): Número de bigramas más frecuentes a identificar por tema.\n",
    "  \n",
    "- **Retorna:**\n",
    "  - `topic_labels` (dict): Diccionario que mapea el ID del tema a su label basado en bigramas.\n",
    "  - `grouped_documents` (dict): Diccionario que agrupa los IDs de documentos por tema.\n",
    "\n",
    "### Creación y Guardado del Modelo LDA\n",
    "\n",
    "El script verifica si un modelo LDA entrenado ya existe en el disco (`lda_model_path`). Si es así, lo carga; de lo contrario, entrena un nuevo modelo LDA utilizando el corpus y lo guarda.\n",
    "\n",
    "## Configuración de la Aplicación Dash\n",
    "\n",
    "### `app.layout`\n",
    "Define el diseño de la aplicación Dash, que incluye:\n",
    "\n",
    "- **Título de la aplicación.**\n",
    "- **Dropdown:** Permite seleccionar un tema basado en los bigramas más frecuentes.\n",
    "- **Div:** Contenedor para mostrar los artículos filtrados.\n",
    "\n",
    "### `update_news_display(selected_topic)`\n",
    "Esta función es un callback que se ejecuta cada vez que se selecciona un tema en el dropdown. Filtra los artículos por el tema seleccionado y muestra los resultados ordenados por fecha.\n",
    "\n",
    "- **Parámetros:**\n",
    "  - `selected_topic` (str): El tema seleccionado en el dropdown.\n",
    "  \n",
    "- **Retorna:**\n",
    "  - `news_articles` (list): Lista de componentes HTML que muestran los artículos de noticias filtrados.\n",
    "\n",
    "## Ejecución de la Aplicación\n",
    "\n",
    "La aplicación se ejecuta utilizando el servidor de Dash:\n",
    "\n",
    "```python\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Andres\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model loaded from disk.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x18d331a8dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from dateutil import parser\n",
    "import os\n",
    "\n",
    "# Ensure required NLTK resources are available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stop words and lemmatizer\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Custom function to parse the Spanish date strings\n",
    "def parse_spanish_date(date_str):\n",
    "    # Handle None or NaN values\n",
    "    if pd.isnull(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Replace month names in Spanish with numbers\n",
    "    months = {\n",
    "        \"enero\": \"01\",\n",
    "        \"febrero\": \"02\",\n",
    "        \"marzo\": \"03\",\n",
    "        \"abril\": \"04\",\n",
    "        \"mayo\": \"05\",\n",
    "        \"junio\": \"06\",\n",
    "        \"julio\": \"07\",\n",
    "        \"agosto\": \"08\",\n",
    "        \"septiembre\": \"09\",\n",
    "        \"octubre\": \"10\",\n",
    "        \"noviembre\": \"11\",\n",
    "        \"diciembre\": \"12\"\n",
    "    }\n",
    "    \n",
    "    date_str = date_str.lower()\n",
    "    \n",
    "    for month, num in months.items():\n",
    "        date_str = date_str.replace(month, num)\n",
    "    \n",
    "    # Now try to parse the date using dateutil.parser\n",
    "    try:\n",
    "        parsed_date = parser.parse(date_str, dayfirst=True)\n",
    "        # Remove timezone info if present\n",
    "        if parsed_date.tzinfo is not None:\n",
    "            parsed_date = parsed_date.replace(tzinfo=None)\n",
    "    except ValueError:\n",
    "        parsed_date = pd.NaT  # Not a Time if parsing fails\n",
    "    \n",
    "    return parsed_date\n",
    "\n",
    "# Apply the custom parsing function to the 'date' column\n",
    "df['DATE'] = df['date'].apply(parse_spanish_date)\n",
    "\n",
    "preprocessed_texts = df['TEXT'].apply(preprocess_text)\n",
    "\n",
    "# Create a dictionary from the preprocessed text\n",
    "dictionary = corpora.Dictionary(preprocessed_texts)\n",
    "\n",
    "# Create a bag-of-words corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in preprocessed_texts]\n",
    "\n",
    "# Define the path to save the LDA model\n",
    "lda_model_path = \"lda_model_interactive.gensim\"\n",
    "\n",
    "# Check if the model already exists, if so, load it; otherwise, train and save it\n",
    "if os.path.exists(lda_model_path):\n",
    "    lda_model = models.LdaModel.load(lda_model_path)\n",
    "    print(\"LDA model loaded from disk.\")\n",
    "else:\n",
    "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    lda_model.save(lda_model_path)\n",
    "    print(\"LDA model trained and saved to disk.\")\n",
    "\n",
    "# Label topics with the 2 most frequent bigrams\n",
    "def identify_topics_by_frequent_bigrams(lda_model, corpus, preprocessed_texts, num_bigrams=2):\n",
    "    \"\"\"Identify each topic by the most frequent bigrams within the documents associated with that topic.\"\"\"\n",
    "    document_topics = [max(lda_model[doc], key=lambda x: x[1])[0] for doc in corpus]\n",
    "    grouped_documents = {topic: [] for topic in range(lda_model.num_topics)}\n",
    "    for doc_id, topic in enumerate(document_topics):\n",
    "        grouped_documents[topic].append(doc_id)\n",
    "    \n",
    "    # Extract the most frequent bigrams for each topic\n",
    "    topic_labels = {}\n",
    "    for topic, documents in grouped_documents.items():\n",
    "        group_texts = [preprocessed_texts[doc_id] for doc_id in documents]\n",
    "        all_bigrams = [bigram for text in group_texts for bigram in ngrams(text, 2)]\n",
    "        bigram_freq = Counter(all_bigrams)\n",
    "        most_common_bigrams = [' '.join(bigram) for bigram, _ in bigram_freq.most_common(num_bigrams)]\n",
    "        topic_labels[topic] = ' / '.join(most_common_bigrams)\n",
    "    \n",
    "    return topic_labels, grouped_documents\n",
    "\n",
    "topic_labels, grouped_documents = identify_topics_by_frequent_bigrams(lda_model, corpus, preprocessed_texts)\n",
    "\n",
    "# Prepare data for filtering and display\n",
    "topic_data = []\n",
    "for doc_id, row in df.iterrows():\n",
    "    topics = lda_model.get_document_topics(corpus[doc_id])\n",
    "    for topic_id, prob in topics:\n",
    "        topic_data.append({\n",
    "            \"Document\": doc_id,\n",
    "            \"Topic\": topic_labels[topic_id],\n",
    "            \"Probability\": prob,\n",
    "            \"Date\": row['DATE'],\n",
    "            \"Text\": row['TEXT']  # Assuming 'TEXT' is the column with the full news article text\n",
    "        })\n",
    "\n",
    "topic_df = pd.DataFrame(topic_data)\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "topic_df['Date'] = pd.to_datetime(topic_df['Date'])\n",
    "\n",
    "# Dash app setup\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"News Article Explorer by Topic\"),\n",
    "    \n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[{'label': topic_labels[i], 'value': topic_labels[i]} for i in range(lda_model.num_topics)],\n",
    "        value=topic_labels[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    \n",
    "    html.Div(id='news-display')\n",
    "])\n",
    "\n",
    "# Callback to update the displayed news articles based on the selected topic\n",
    "@app.callback(\n",
    "    Output('news-display', 'children'),\n",
    "    [Input('topic-dropdown', 'value')]\n",
    ")\n",
    "def update_news_display(selected_topic):\n",
    "    filtered_df = topic_df[(topic_df['Topic'] == selected_topic) & (topic_df['Probability'] > 0.5)]\n",
    "    filtered_df = filtered_df.sort_values(by='Date', ascending=True)\n",
    "    \n",
    "    news_articles = []\n",
    "    for _, row in filtered_df.iterrows():\n",
    "        # Handle NaT values by using a placeholder or empty string\n",
    "        date_str = row['Date'].strftime('%Y-%m-%d') if pd.notnull(row['Date']) else \"Unknown Date\"\n",
    "        \n",
    "        news_articles.append(html.Div([\n",
    "            html.H3(f\"Date: {date_str}\"),\n",
    "            html.P(row['Text']),\n",
    "            html.Hr()\n",
    "        ]))\n",
    "    \n",
    "    return news_articles\n",
    "\n",
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hl7-speak2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
